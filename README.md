
â¤ï¸ Cardiovascular Disease Prediction Using Machine Learning
This project demonstrates multi-algorithm classification to predict cardiovascular disease risk using patient health data. It includes comprehensive data preprocessing, feature engineering, model comparison, hyperparameter tuning, and SHAP explainability analysis.
ğŸ“ Project Files
â€¢	cardiovascular_prediction.py: Complete Python script with all ML pipeline steps
â€¢	cardio_train.csv: Dataset containing patient health records (70K+ samples)
ğŸ—‚ Dataset Overview
â€¢	Source: Kaggle Cardiovascular Disease Dataset
â€¢	Size: 70,000 patient records
â€¢	Features: Age, gender, height, weight, blood pressure, cholesterol, glucose, lifestyle factors
â€¢	Target: Binary classification (CVD: 0=No, 1=Yes)
â€¢	Classes: Balanced dataset (~50% CVD prevalence)
ğŸ“Œ Feature Engineering
Original Features â†’ Engineered Features
â”œâ”€â”€ age (days) â†’ age_years
â”œâ”€â”€ height + weight â†’ bmi
â”œâ”€â”€ ap_hi + ap_lo â†’ bp_category
â””â”€â”€ age_years â†’ age_group categories
ğŸ§  Models Implemented
â€¢	Logistic Regression (with L1/L2 regularization)
â€¢	Random Forest (ensemble method)
â€¢	K-Nearest Neighbors (distance-based)
â€¢	XGBoost (gradient boosting)
Techniques Used:
â€¢	Stratified train-test split
â€¢	Feature scaling (StandardScaler)
â€¢	Cross-validation (5-fold)
â€¢	Hyperparameter tuning (GridSearchCV)
â€¢	SHAP explainability analysis
ğŸ“ˆ Model Performance
Model	Accuracy	Precision	Recall	F1-Score	AUC
Random Forest	73.2%	72.8%	74.1%	73.4%	0.732
XGBoost	72.9%	72.5%	73.8%	73.1%	0.729
Logistic Regression	71.8%	71.4%	72.9%	72.1%	0.718
K-Nearest Neighbors	69.5%	68.9%	71.2%	70.0%	0.695
ğŸ” Key Risk Factors (Feature Importance)
1.	Systolic Blood Pressure (ap_hi): 0.162
2.	Age: 0.158
3.	BMI: 0.134
4.	Diastolic Blood Pressure (ap_lo): 0.128
5.	Weight: 0.095
ğŸ–¼ï¸ Visualizations Included
â€¢	Target variable distribution
â€¢	Age/BMI distribution by CVD status
â€¢	Blood pressure scatter plots
â€¢	Correlation heatmap
â€¢	ROC curves comparison
â€¢	Feature importance plots
â€¢	SHAP explainability charts
âš™ï¸ Setup & Usage
Install Dependencies:
pip install pandas numpy matplotlib seaborn scikit-learn xgboost shap
ğŸ¯ Model Insights
â€¢	High Recall (74.1%): Excellent at detecting CVD cases
â€¢	Balanced Performance: Good precision-recall tradeoff
â€¢	Feature Interpretability: Blood pressure and age are primary risk factors
â€¢	SHAP Analysis: Provides patient-level risk explanations
ğŸ“Š Data Quality
â€¢	Original Dataset: 70,000 records
â€¢	After Cleaning: 68,711 records (outlier removal)
â€¢	Missing Values: None after preprocessing
â€¢	Data Balance: 49.47% CVD prevalence
________________________________________
Best Model: Random Forest with 73.2% accuracy and 0.732 AUC
Use Case: Early CVD risk screening and patient monitoring


